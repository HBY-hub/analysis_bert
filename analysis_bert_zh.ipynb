{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analysis_bert_zh.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4928b226cdde4ba7852577b9c9209ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_19b0b292f06c4ba3a53490f2dfea2fe2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_21501e1c95e649e3a736e4ec71b07abe",
              "IPY_MODEL_beac9a97c9674c788f2ac4b1611be146"
            ]
          }
        },
        "19b0b292f06c4ba3a53490f2dfea2fe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21501e1c95e649e3a736e4ec71b07abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ca356d229a16422887ee7a30c9ee82a7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 109540,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 109540,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41e964dc495d4b1eaf249adf63e37075"
          }
        },
        "beac9a97c9674c788f2ac4b1611be146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7d1483934eb14fe1b641d26fb35628f6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 110k/110k [00:00&lt;00:00, 636kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_74088ebcd1e34c0d90127ec64f23f0b5"
          }
        },
        "ca356d229a16422887ee7a30c9ee82a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41e964dc495d4b1eaf249adf63e37075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d1483934eb14fe1b641d26fb35628f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "74088ebcd1e34c0d90127ec64f23f0b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03f5c7a5c8704258a17c1e23780be93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7ef3c83710c34ebe8ce65703fdc232cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c7923bf576948b29f676ade5dee0e8f",
              "IPY_MODEL_24a25ffff380449e87844d7b4f5317ae"
            ]
          }
        },
        "7ef3c83710c34ebe8ce65703fdc232cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c7923bf576948b29f676ade5dee0e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_258f02f2b5d547f0b4e9536e2419ea58",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 624,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 624,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c836c2eb42334b0eaf5912d7810cf5a4"
          }
        },
        "24a25ffff380449e87844d7b4f5317ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0d6ad60cf7044ad8445b3f476f20b87",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 624/624 [00:05&lt;00:00, 108B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cabb4b59b4794501aaed724d3af38c86"
          }
        },
        "258f02f2b5d547f0b4e9536e2419ea58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c836c2eb42334b0eaf5912d7810cf5a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0d6ad60cf7044ad8445b3f476f20b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cabb4b59b4794501aaed724d3af38c86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aba6695f792240a58e16e5ec001f78b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_896c40a2bd144352b22cbc2e7f330a8c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e3bc71e7f9bf41f69429b1ca8bf455ff",
              "IPY_MODEL_1bbb5340d29c46b4a581c54a3be589e0"
            ]
          }
        },
        "896c40a2bd144352b22cbc2e7f330a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3bc71e7f9bf41f69429b1ca8bf455ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6089e16d8ae749b6b460108d67c7c65a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 411577189,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 411577189,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c00d623adb824bd99c6a4b083298f944"
          }
        },
        "1bbb5340d29c46b4a581c54a3be589e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2bd797c0d6eb4253813fa7a420d72f52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 412M/412M [00:07&lt;00:00, 55.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ce53900e3244ee49c283d2b3cf2f472"
          }
        },
        "6089e16d8ae749b6b460108d67c7c65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c00d623adb824bd99c6a4b083298f944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bd797c0d6eb4253813fa7a420d72f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ce53900e3244ee49c283d2b3cf2f472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMixvKQAuRq9",
        "colab_type": "text"
      },
      "source": [
        "# BERT 中文分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tGLdYF-uT48",
        "colab_type": "text"
      },
      "source": [
        "## 查看GPU是否能用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U30tul4YDmno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7e78540b-d0ba-4dcb-cc3b-78874b5c766b"
      },
      "source": [
        "# 看一看用的什么显卡\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Sep  3 06:43:59 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzriA5rjuBQO",
        "colab_type": "text"
      },
      "source": [
        "## 安装transformers库，并解压数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQgXyYO2Dvcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!unzip /content/drive/Shared\\ drives/hbyscgsg@gmail.com/weibo_senti_100k.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EugO-_A-EMVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import pandas as pd"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JHZCUGFudlL",
        "colab_type": "text"
      },
      "source": [
        "## 通过pandas载入数据集并展示样例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEPz46ZQERMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "b45a4c63-0863-425e-9b61-3addc9923493"
      },
      "source": [
        "pd_all = pd.read_csv(\"/content/weibo_senti_100k.csv\")\n",
        "print('评论数目（总体）：%d' % pd_all.shape[0])\n",
        "print('评论数目（正向）：%d' % pd_all[pd_all.label==1].shape[0])\n",
        "print('评论数目（负向）：%d' % pd_all[pd_all.label==0].shape[0])\n",
        "pd_all.sample(20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "评论数目（总体）：119988\n",
            "评论数目（正向）：59993\n",
            "评论数目（负向）：59995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>51490</th>\n",
              "      <td>1</td>\n",
              "      <td>我不确定能否活到那会[哼] //@c哈哈:我也来凑个热闹，@大大大老沈 和你永远幸福哦~~~...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20456</th>\n",
              "      <td>1</td>\n",
              "      <td>又长姿势了！意大利面的各种名称以及对照图~不懂的mark下~[爱你]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30151</th>\n",
              "      <td>1</td>\n",
              "      <td>台湾水果太好吃了，连我不爱吃的小番茄都这么好粗[鼓掌][鼓掌]走高速吃水果好苏胡[馋嘴][馋...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18492</th>\n",
              "      <td>1</td>\n",
              "      <td>//@上海中期许思达:// @DJ隋怡 : [哈哈]// @CRI刘彦 : 您咋这么经典？[...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59939</th>\n",
              "      <td>1</td>\n",
              "      <td>幸福幸福无比幸福[哈哈]//@郝郝的好妈妈:@雨过天晴之霁_泽雯 @左咖_晒着太阳儿 @北京鸭脖</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84242</th>\n",
              "      <td>0</td>\n",
              "      <td>一不小心，把今天当成父亲节啦[晕]，既备之，则做之！为了今晚的Pre-Fathers Day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66456</th>\n",
              "      <td>0</td>\n",
              "      <td>每项开销都留发票是个好习惯，起码你能粗算一下月平均个人花费是多少[汗]（问题是就我这数学耐力...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56156</th>\n",
              "      <td>1</td>\n",
              "      <td>JIN AIR，真航空，真爱你。 //@大韩航空:那一抹清新亮丽的风景和纯真的笑容，JIN ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20494</th>\n",
              "      <td>1</td>\n",
              "      <td>哈哈哈小礼物[哈哈][哈哈][哈哈]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98809</th>\n",
              "      <td>0</td>\n",
              "      <td>我咋活的这么纠结啊[泪]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1356</th>\n",
              "      <td>1</td>\n",
              "      <td>好的。[嘻嘻] //@桔子水晶袁超:张贴到员工天地，例会时要表扬，要广而告之[鼓掌][鼓掌]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31830</th>\n",
              "      <td>1</td>\n",
              "      <td>我那两个同学在大学的外号一个叫猴子，另外一个算瘦子！[太开心]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44378</th>\n",
              "      <td>1</td>\n",
              "      <td>待续//@东方盛悦: [哈哈]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83661</th>\n",
              "      <td>0</td>\n",
              "      <td>又全中。。。。[泪]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109797</th>\n",
              "      <td>0</td>\n",
              "      <td>回复@Joe霞客行:[馋嘴][抓狂]霞客你等着！ //@Joe霞客行:家乡特产，来吧，我请!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13304</th>\n",
              "      <td>1</td>\n",
              "      <td>//@日本中部: 萌死不偿命啊！[爱你]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106093</th>\n",
              "      <td>0</td>\n",
              "      <td>巧克力馅的汤圆和牛肉饺子一起煮，坏了。。成了巧克力牛肉汤圆饺子片汤粥。。不知道叫什么好了。。...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50896</th>\n",
              "      <td>1</td>\n",
              "      <td>@Becky无敌小墩墩 @Sunshiny_雪盈  好开心的一天 累并快乐着 有你们真好！这...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663</th>\n",
              "      <td>1</td>\n",
              "      <td>[嘻嘻][嘻嘻] //@Cafe-玄磊:糖哥你别这样说啦[做鬼脸] //@糖哥-咖啡大佬老冯...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5416</th>\n",
              "      <td>1</td>\n",
              "      <td>美食美刻波尔多，主持人小皮老师各种搞笑，各种插科打诨，各种精彩，刘老师，侬错过啦[哈哈]@葡...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        label                                             review\n",
              "51490       1  我不确定能否活到那会[哼] //@c哈哈:我也来凑个热闹，@大大大老沈 和你永远幸福哦~~~...\n",
              "20456       1                 又长姿势了！意大利面的各种名称以及对照图~不懂的mark下~[爱你]\n",
              "30151       1  台湾水果太好吃了，连我不爱吃的小番茄都这么好粗[鼓掌][鼓掌]走高速吃水果好苏胡[馋嘴][馋...\n",
              "18492       1  //@上海中期许思达:// @DJ隋怡 : [哈哈]// @CRI刘彦 : 您咋这么经典？[...\n",
              "59939       1   幸福幸福无比幸福[哈哈]//@郝郝的好妈妈:@雨过天晴之霁_泽雯 @左咖_晒着太阳儿 @北京鸭脖\n",
              "84242       0  一不小心，把今天当成父亲节啦[晕]，既备之，则做之！为了今晚的Pre-Fathers Day...\n",
              "66456       0  每项开销都留发票是个好习惯，起码你能粗算一下月平均个人花费是多少[汗]（问题是就我这数学耐力...\n",
              "56156       1  JIN AIR，真航空，真爱你。 //@大韩航空:那一抹清新亮丽的风景和纯真的笑容，JIN ...\n",
              "20494       1                                 哈哈哈小礼物[哈哈][哈哈][哈哈]\n",
              "98809       0                                       我咋活的这么纠结啊[泪]\n",
              "1356        1  好的。[嘻嘻] //@桔子水晶袁超:张贴到员工天地，例会时要表扬，要广而告之[鼓掌][鼓掌]...\n",
              "31830       1                    我那两个同学在大学的外号一个叫猴子，另外一个算瘦子！[太开心]\n",
              "44378       1                                    待续//@东方盛悦: [哈哈]\n",
              "83661       0                                         又全中。。。。[泪]\n",
              "109797      0  回复@Joe霞客行:[馋嘴][抓狂]霞客你等着！ //@Joe霞客行:家乡特产，来吧，我请!...\n",
              "13304       1                               //@日本中部: 萌死不偿命啊！[爱你]\n",
              "106093      0  巧克力馅的汤圆和牛肉饺子一起煮，坏了。。成了巧克力牛肉汤圆饺子片汤粥。。不知道叫什么好了。。...\n",
              "50896       1  @Becky无敌小墩墩 @Sunshiny_雪盈  好开心的一天 累并快乐着 有你们真好！这...\n",
              "3663        1  [嘻嘻][嘻嘻] //@Cafe-玄磊:糖哥你别这样说啦[做鬼脸] //@糖哥-咖啡大佬老冯...\n",
              "5416        1  美食美刻波尔多，主持人小皮老师各种搞笑，各种插科打诨，各种精彩，刘老师，侬错过啦[哈哈]@葡..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fjYmAfeuzOE",
        "colab_type": "text"
      },
      "source": [
        "## 对数据预处理下"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4IoiT05KpWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = int(0.8 * 119988)\n",
        "test_size = 119988 - train_size\n",
        "train_indices,test_indices = torch.utils.data.random_split(pd_all,[train_size,test_size])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiMCdh9uT4Ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = pd_all['review']\n",
        "labels = pd_all['label']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkze4riQIChd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels=[]\n",
        "for i in train_indices.indices:\n",
        "  train_labels.append(labels[i])\n",
        "\n",
        "train_sentences=[]\n",
        "for i in train_indices.indices:\n",
        "  train_sentences.append(sentences[i])\n",
        "\n",
        "test_labels=[]\n",
        "for i in test_indices.indices:\n",
        "  test_labels.append(labels[i])\n",
        "\n",
        "test_sentences=[]\n",
        "for i in test_indices.indices:\n",
        "  test_sentences.append(sentences[i])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAf3G-gsXGIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a4074767-c43d-4018-c870-d577f889acc9"
      },
      "source": [
        "len(train_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95990"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_C0SHtvvtaz",
        "colab_type": "text"
      },
      "source": [
        "## 载入BERT中文的tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l6e-79gGhPR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4928b226cdde4ba7852577b9c9209ce1",
            "19b0b292f06c4ba3a53490f2dfea2fe2",
            "21501e1c95e649e3a736e4ec71b07abe",
            "beac9a97c9674c788f2ac4b1611be146",
            "ca356d229a16422887ee7a30c9ee82a7",
            "41e964dc495d4b1eaf249adf63e37075",
            "7d1483934eb14fe1b641d26fb35628f6",
            "74088ebcd1e34c0d90127ec64f23f0b5"
          ]
        },
        "outputId": "178f6e11-d4de-4423-d4f7-c5a76f116423"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "bert_model = \"bert-base-chinese\"\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4928b226cdde4ba7852577b9c9209ce1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=109540.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3j0nZv-JXTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "b685463d-787d-4cc3-9bdc-9c7a70dd80e9"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', train_sentences[1])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(train_sentences[1]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[1])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Life isnt about waiting for the storm to pass, Its about learning to dance in the rain//@娜小鱼儿:早安，听首好歌，干活去喽~[抱抱]\n",
            "Tokenized:  ['[UNK]', 'is', '##nt', 'about', 'w', '##ai', '##ting', 'for', 'the', 'st', '##orm', 'to', 'pass', ',', '[UNK]', 'about', 'learning', 'to', 'dance', 'in', 'the', 'rain', '/', '/', '@', '娜', '小', '鱼', '儿', ':', '早', '安', '，', '听', '首', '好', '歌', '，', '干', '活', '去', '喽', '~', '[', '抱', '抱', ']']\n",
            "Token IDs:  [100, 8310, 8511, 9053, 165, 8982, 9107, 8330, 8174, 8811, 10530, 8228, 9703, 117, 100, 9053, 12315, 8228, 12371, 8217, 8174, 11873, 120, 120, 137, 2025, 2207, 7824, 1036, 131, 3193, 2128, 8024, 1420, 7674, 1962, 3625, 8024, 2397, 3833, 1343, 1617, 172, 138, 2849, 2849, 140]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDRVUlIav5sb",
        "colab_type": "text"
      },
      "source": [
        "## 对每句话转换成的序列做预处理\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN72SAl2x6Dz",
        "colab_type": "text"
      },
      "source": [
        "MAX_LEN设置为128\n",
        "\n",
        "好像没有超过128的\n",
        "\n",
        "没达到自动填充"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-YtJMlCJsEg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d48dcf81-c4d0-4c76-f55c-f5c564c87723"
      },
      "source": [
        "MAX_LEN=128\n",
        "\n",
        "input_ids = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN,truncation=True) for sent in train_sentences]\n",
        "test_input_ids = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN,truncation=True) for sent in test_sentences]\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "print(\"finish\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-eqwWZqwh4c",
        "colab_type": "text"
      },
      "source": [
        "## 设置每句话的的mask给BERT训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goVTniO1MLMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "    \n",
        "test_attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in test_input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    test_attention_masks.append(att_mask)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y09bXfZwtxk",
        "colab_type": "text"
      },
      "source": [
        "## 对数据集划分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jchFAt9AMOb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "labels = train_labels\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2020, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2020, test_size=0.1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2jRdWlfw2Aa",
        "colab_type": "text"
      },
      "source": [
        "## 创建dataloader，是数据分批次进行训练\n",
        "防止内存不足产生问题\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZEnMJ9NMhox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "test_inputs=torch.tensor(test_input_ids)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "test_labels=torch.tensor(test_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "test_masks=torch.tensor(test_attention_masks)\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our test set.\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAGVMn8ZxJI5",
        "colab_type": "text"
      },
      "source": [
        "## 创建用于分类中文的BERT模型（二分类）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr0xxFmuMr3v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "03f5c7a5c8704258a17c1e23780be93a",
            "7ef3c83710c34ebe8ce65703fdc232cf",
            "1c7923bf576948b29f676ade5dee0e8f",
            "24a25ffff380449e87844d7b4f5317ae",
            "258f02f2b5d547f0b4e9536e2419ea58",
            "c836c2eb42334b0eaf5912d7810cf5a4",
            "d0d6ad60cf7044ad8445b3f476f20b87",
            "cabb4b59b4794501aaed724d3af38c86",
            "aba6695f792240a58e16e5ec001f78b6",
            "896c40a2bd144352b22cbc2e7f330a8c",
            "e3bc71e7f9bf41f69429b1ca8bf455ff",
            "1bbb5340d29c46b4a581c54a3be589e0",
            "6089e16d8ae749b6b460108d67c7c65a",
            "c00d623adb824bd99c6a4b083298f944",
            "2bd797c0d6eb4253813fa7a420d72f52",
            "8ce53900e3244ee49c283d2b3cf2f472"
          ]
        },
        "outputId": "fbbeddba-d5ec-44d2-ebe2-0377d0b54b06"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-chinese\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03f5c7a5c8704258a17c1e23780be93a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=624.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aba6695f792240a58e16e5ec001f78b6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411577189.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFn7aYixxT-R",
        "colab_type": "text"
      },
      "source": [
        "## 创建优化器，以及制定一些参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHoBwzoCMySH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uaqm9JMxc1i",
        "colab_type": "text"
      },
      "source": [
        "## 创建评估函数和一个格式化时间的函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R-DEB70M0sH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToiReki5xkeT",
        "colab_type": "text"
      },
      "source": [
        "## 开始训练！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAM8m2XeM4XX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b876e26e-28a3-4236-a52d-acd9df19ce7d"
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):# epoches\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  5,400.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  5,400.    Elapsed: 0:00:30.\n",
            "  Batch   120  of  5,400.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  5,400.    Elapsed: 0:01:02.\n",
            "  Batch   200  of  5,400.    Elapsed: 0:01:19.\n",
            "  Batch   240  of  5,400.    Elapsed: 0:01:35.\n",
            "  Batch   280  of  5,400.    Elapsed: 0:01:51.\n",
            "  Batch   320  of  5,400.    Elapsed: 0:02:07.\n",
            "  Batch   360  of  5,400.    Elapsed: 0:02:23.\n",
            "  Batch   400  of  5,400.    Elapsed: 0:02:39.\n",
            "  Batch   440  of  5,400.    Elapsed: 0:02:55.\n",
            "  Batch   480  of  5,400.    Elapsed: 0:03:11.\n",
            "  Batch   520  of  5,400.    Elapsed: 0:03:27.\n",
            "  Batch   560  of  5,400.    Elapsed: 0:03:43.\n",
            "  Batch   600  of  5,400.    Elapsed: 0:04:00.\n",
            "  Batch   640  of  5,400.    Elapsed: 0:04:16.\n",
            "  Batch   680  of  5,400.    Elapsed: 0:04:32.\n",
            "  Batch   720  of  5,400.    Elapsed: 0:04:48.\n",
            "  Batch   760  of  5,400.    Elapsed: 0:05:04.\n",
            "  Batch   800  of  5,400.    Elapsed: 0:05:21.\n",
            "  Batch   840  of  5,400.    Elapsed: 0:05:37.\n",
            "  Batch   880  of  5,400.    Elapsed: 0:05:53.\n",
            "  Batch   920  of  5,400.    Elapsed: 0:06:09.\n",
            "  Batch   960  of  5,400.    Elapsed: 0:06:25.\n",
            "  Batch 1,000  of  5,400.    Elapsed: 0:06:41.\n",
            "  Batch 1,040  of  5,400.    Elapsed: 0:06:57.\n",
            "  Batch 1,080  of  5,400.    Elapsed: 0:07:13.\n",
            "  Batch 1,120  of  5,400.    Elapsed: 0:07:30.\n",
            "  Batch 1,160  of  5,400.    Elapsed: 0:07:46.\n",
            "  Batch 1,200  of  5,400.    Elapsed: 0:08:02.\n",
            "  Batch 1,240  of  5,400.    Elapsed: 0:08:18.\n",
            "  Batch 1,280  of  5,400.    Elapsed: 0:08:34.\n",
            "  Batch 1,320  of  5,400.    Elapsed: 0:08:50.\n",
            "  Batch 1,360  of  5,400.    Elapsed: 0:09:06.\n",
            "  Batch 1,400  of  5,400.    Elapsed: 0:09:23.\n",
            "  Batch 1,440  of  5,400.    Elapsed: 0:09:39.\n",
            "  Batch 1,480  of  5,400.    Elapsed: 0:09:55.\n",
            "  Batch 1,520  of  5,400.    Elapsed: 0:10:11.\n",
            "  Batch 1,560  of  5,400.    Elapsed: 0:10:27.\n",
            "  Batch 1,600  of  5,400.    Elapsed: 0:10:43.\n",
            "  Batch 1,640  of  5,400.    Elapsed: 0:11:00.\n",
            "  Batch 1,680  of  5,400.    Elapsed: 0:11:16.\n",
            "  Batch 1,720  of  5,400.    Elapsed: 0:11:32.\n",
            "  Batch 1,760  of  5,400.    Elapsed: 0:11:48.\n",
            "  Batch 1,800  of  5,400.    Elapsed: 0:12:04.\n",
            "  Batch 1,840  of  5,400.    Elapsed: 0:12:20.\n",
            "  Batch 1,880  of  5,400.    Elapsed: 0:12:36.\n",
            "  Batch 1,920  of  5,400.    Elapsed: 0:12:53.\n",
            "  Batch 1,960  of  5,400.    Elapsed: 0:13:09.\n",
            "  Batch 2,000  of  5,400.    Elapsed: 0:13:25.\n",
            "  Batch 2,040  of  5,400.    Elapsed: 0:13:41.\n",
            "  Batch 2,080  of  5,400.    Elapsed: 0:13:57.\n",
            "  Batch 2,120  of  5,400.    Elapsed: 0:14:13.\n",
            "  Batch 2,160  of  5,400.    Elapsed: 0:14:29.\n",
            "  Batch 2,200  of  5,400.    Elapsed: 0:14:46.\n",
            "  Batch 2,240  of  5,400.    Elapsed: 0:15:02.\n",
            "  Batch 2,280  of  5,400.    Elapsed: 0:15:18.\n",
            "  Batch 2,320  of  5,400.    Elapsed: 0:15:34.\n",
            "  Batch 2,360  of  5,400.    Elapsed: 0:15:50.\n",
            "  Batch 2,400  of  5,400.    Elapsed: 0:16:06.\n",
            "  Batch 2,440  of  5,400.    Elapsed: 0:16:22.\n",
            "  Batch 2,480  of  5,400.    Elapsed: 0:16:39.\n",
            "  Batch 2,520  of  5,400.    Elapsed: 0:16:55.\n",
            "  Batch 2,560  of  5,400.    Elapsed: 0:17:11.\n",
            "  Batch 2,600  of  5,400.    Elapsed: 0:17:27.\n",
            "  Batch 2,640  of  5,400.    Elapsed: 0:17:43.\n",
            "  Batch 2,680  of  5,400.    Elapsed: 0:18:00.\n",
            "  Batch 2,720  of  5,400.    Elapsed: 0:18:16.\n",
            "  Batch 2,760  of  5,400.    Elapsed: 0:18:32.\n",
            "  Batch 2,800  of  5,400.    Elapsed: 0:18:48.\n",
            "  Batch 2,840  of  5,400.    Elapsed: 0:19:04.\n",
            "  Batch 2,880  of  5,400.    Elapsed: 0:19:20.\n",
            "  Batch 2,920  of  5,400.    Elapsed: 0:19:36.\n",
            "  Batch 2,960  of  5,400.    Elapsed: 0:19:53.\n",
            "  Batch 3,000  of  5,400.    Elapsed: 0:20:09.\n",
            "  Batch 3,040  of  5,400.    Elapsed: 0:20:25.\n",
            "  Batch 3,080  of  5,400.    Elapsed: 0:20:41.\n",
            "  Batch 3,120  of  5,400.    Elapsed: 0:20:57.\n",
            "  Batch 3,160  of  5,400.    Elapsed: 0:21:13.\n",
            "  Batch 3,200  of  5,400.    Elapsed: 0:21:29.\n",
            "  Batch 3,240  of  5,400.    Elapsed: 0:21:45.\n",
            "  Batch 3,280  of  5,400.    Elapsed: 0:22:01.\n",
            "  Batch 3,320  of  5,400.    Elapsed: 0:22:17.\n",
            "  Batch 3,360  of  5,400.    Elapsed: 0:22:34.\n",
            "  Batch 3,400  of  5,400.    Elapsed: 0:22:50.\n",
            "  Batch 3,440  of  5,400.    Elapsed: 0:23:06.\n",
            "  Batch 3,480  of  5,400.    Elapsed: 0:23:22.\n",
            "  Batch 3,520  of  5,400.    Elapsed: 0:23:38.\n",
            "  Batch 3,560  of  5,400.    Elapsed: 0:23:54.\n",
            "  Batch 3,600  of  5,400.    Elapsed: 0:24:10.\n",
            "  Batch 3,640  of  5,400.    Elapsed: 0:24:26.\n",
            "  Batch 3,680  of  5,400.    Elapsed: 0:24:42.\n",
            "  Batch 3,720  of  5,400.    Elapsed: 0:24:59.\n",
            "  Batch 3,760  of  5,400.    Elapsed: 0:25:15.\n",
            "  Batch 3,800  of  5,400.    Elapsed: 0:25:31.\n",
            "  Batch 3,840  of  5,400.    Elapsed: 0:25:47.\n",
            "  Batch 3,880  of  5,400.    Elapsed: 0:26:03.\n",
            "  Batch 3,920  of  5,400.    Elapsed: 0:26:19.\n",
            "  Batch 3,960  of  5,400.    Elapsed: 0:26:35.\n",
            "  Batch 4,000  of  5,400.    Elapsed: 0:26:51.\n",
            "  Batch 4,040  of  5,400.    Elapsed: 0:27:07.\n",
            "  Batch 4,080  of  5,400.    Elapsed: 0:27:24.\n",
            "  Batch 4,120  of  5,400.    Elapsed: 0:27:40.\n",
            "  Batch 4,160  of  5,400.    Elapsed: 0:27:56.\n",
            "  Batch 4,200  of  5,400.    Elapsed: 0:28:12.\n",
            "  Batch 4,240  of  5,400.    Elapsed: 0:28:28.\n",
            "  Batch 4,280  of  5,400.    Elapsed: 0:28:44.\n",
            "  Batch 4,320  of  5,400.    Elapsed: 0:29:00.\n",
            "  Batch 4,360  of  5,400.    Elapsed: 0:29:16.\n",
            "  Batch 4,400  of  5,400.    Elapsed: 0:29:32.\n",
            "  Batch 4,440  of  5,400.    Elapsed: 0:29:48.\n",
            "  Batch 4,480  of  5,400.    Elapsed: 0:30:04.\n",
            "  Batch 4,520  of  5,400.    Elapsed: 0:30:21.\n",
            "  Batch 4,560  of  5,400.    Elapsed: 0:30:37.\n",
            "  Batch 4,600  of  5,400.    Elapsed: 0:30:53.\n",
            "  Batch 4,640  of  5,400.    Elapsed: 0:31:09.\n",
            "  Batch 4,680  of  5,400.    Elapsed: 0:31:25.\n",
            "  Batch 4,720  of  5,400.    Elapsed: 0:31:41.\n",
            "  Batch 4,760  of  5,400.    Elapsed: 0:31:58.\n",
            "  Batch 4,800  of  5,400.    Elapsed: 0:32:14.\n",
            "  Batch 4,840  of  5,400.    Elapsed: 0:32:30.\n",
            "  Batch 4,880  of  5,400.    Elapsed: 0:32:46.\n",
            "  Batch 4,920  of  5,400.    Elapsed: 0:33:02.\n",
            "  Batch 4,960  of  5,400.    Elapsed: 0:33:18.\n",
            "  Batch 5,000  of  5,400.    Elapsed: 0:33:34.\n",
            "  Batch 5,040  of  5,400.    Elapsed: 0:33:50.\n",
            "  Batch 5,080  of  5,400.    Elapsed: 0:34:06.\n",
            "  Batch 5,120  of  5,400.    Elapsed: 0:34:23.\n",
            "  Batch 5,160  of  5,400.    Elapsed: 0:34:39.\n",
            "  Batch 5,200  of  5,400.    Elapsed: 0:34:55.\n",
            "  Batch 5,240  of  5,400.    Elapsed: 0:35:11.\n",
            "  Batch 5,280  of  5,400.    Elapsed: 0:35:27.\n",
            "  Batch 5,320  of  5,400.    Elapsed: 0:35:43.\n",
            "  Batch 5,360  of  5,400.    Elapsed: 0:35:59.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:36:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:01:25\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  5,400.    Elapsed: 0:00:16.\n",
            "  Batch    80  of  5,400.    Elapsed: 0:00:32.\n",
            "  Batch   120  of  5,400.    Elapsed: 0:00:48.\n",
            "  Batch   160  of  5,400.    Elapsed: 0:01:04.\n",
            "  Batch   200  of  5,400.    Elapsed: 0:01:21.\n",
            "  Batch   280  of  5,400.    Elapsed: 0:01:53.\n",
            "  Batch   320  of  5,400.    Elapsed: 0:02:09.\n",
            "  Batch   360  of  5,400.    Elapsed: 0:02:25.\n",
            "  Batch   400  of  5,400.    Elapsed: 0:02:41.\n",
            "  Batch   440  of  5,400.    Elapsed: 0:02:57.\n",
            "  Batch   480  of  5,400.    Elapsed: 0:03:13.\n",
            "  Batch   520  of  5,400.    Elapsed: 0:03:30.\n",
            "  Batch   560  of  5,400.    Elapsed: 0:03:46.\n",
            "  Batch   600  of  5,400.    Elapsed: 0:04:02.\n",
            "  Batch   640  of  5,400.    Elapsed: 0:04:18.\n",
            "  Batch   680  of  5,400.    Elapsed: 0:04:34.\n",
            "  Batch   720  of  5,400.    Elapsed: 0:04:50.\n",
            "  Batch   760  of  5,400.    Elapsed: 0:05:06.\n",
            "  Batch   800  of  5,400.    Elapsed: 0:05:22.\n",
            "  Batch   840  of  5,400.    Elapsed: 0:05:38.\n",
            "  Batch   880  of  5,400.    Elapsed: 0:05:54.\n",
            "  Batch   920  of  5,400.    Elapsed: 0:06:10.\n",
            "  Batch   960  of  5,400.    Elapsed: 0:06:26.\n",
            "  Batch 1,000  of  5,400.    Elapsed: 0:06:42.\n",
            "  Batch 1,040  of  5,400.    Elapsed: 0:06:58.\n",
            "  Batch 1,080  of  5,400.    Elapsed: 0:07:15.\n",
            "  Batch 1,120  of  5,400.    Elapsed: 0:07:31.\n",
            "  Batch 1,160  of  5,400.    Elapsed: 0:07:47.\n",
            "  Batch 1,200  of  5,400.    Elapsed: 0:08:03.\n",
            "  Batch 1,240  of  5,400.    Elapsed: 0:08:19.\n",
            "  Batch 1,280  of  5,400.    Elapsed: 0:08:35.\n",
            "  Batch 1,320  of  5,400.    Elapsed: 0:08:51.\n",
            "  Batch 1,360  of  5,400.    Elapsed: 0:09:07.\n",
            "  Batch 1,400  of  5,400.    Elapsed: 0:09:23.\n",
            "  Batch 1,440  of  5,400.    Elapsed: 0:09:39.\n",
            "  Batch 1,480  of  5,400.    Elapsed: 0:09:55.\n",
            "  Batch 1,520  of  5,400.    Elapsed: 0:10:11.\n",
            "  Batch 1,560  of  5,400.    Elapsed: 0:10:27.\n",
            "  Batch 1,600  of  5,400.    Elapsed: 0:10:43.\n",
            "  Batch 1,640  of  5,400.    Elapsed: 0:11:00.\n",
            "  Batch 1,680  of  5,400.    Elapsed: 0:11:16.\n",
            "  Batch 1,720  of  5,400.    Elapsed: 0:11:32.\n",
            "  Batch 1,760  of  5,400.    Elapsed: 0:11:48.\n",
            "  Batch 1,800  of  5,400.    Elapsed: 0:12:04.\n",
            "  Batch 1,840  of  5,400.    Elapsed: 0:12:20.\n",
            "  Batch 1,880  of  5,400.    Elapsed: 0:12:36.\n",
            "  Batch 1,920  of  5,400.    Elapsed: 0:12:53.\n",
            "  Batch 1,960  of  5,400.    Elapsed: 0:13:09.\n",
            "  Batch 2,000  of  5,400.    Elapsed: 0:13:25.\n",
            "  Batch 2,040  of  5,400.    Elapsed: 0:13:41.\n",
            "  Batch 2,080  of  5,400.    Elapsed: 0:13:57.\n",
            "  Batch 2,120  of  5,400.    Elapsed: 0:14:13.\n",
            "  Batch 2,160  of  5,400.    Elapsed: 0:14:29.\n",
            "  Batch 2,200  of  5,400.    Elapsed: 0:14:45.\n",
            "  Batch 2,240  of  5,400.    Elapsed: 0:15:01.\n",
            "  Batch 2,280  of  5,400.    Elapsed: 0:15:17.\n",
            "  Batch 2,320  of  5,400.    Elapsed: 0:15:33.\n",
            "  Batch 2,360  of  5,400.    Elapsed: 0:15:49.\n",
            "  Batch 2,400  of  5,400.    Elapsed: 0:16:06.\n",
            "  Batch 2,440  of  5,400.    Elapsed: 0:16:22.\n",
            "  Batch 2,480  of  5,400.    Elapsed: 0:16:38.\n",
            "  Batch 2,520  of  5,400.    Elapsed: 0:16:54.\n",
            "  Batch 2,560  of  5,400.    Elapsed: 0:17:10.\n",
            "  Batch 2,600  of  5,400.    Elapsed: 0:17:26.\n",
            "  Batch 2,640  of  5,400.    Elapsed: 0:17:42.\n",
            "  Batch 2,680  of  5,400.    Elapsed: 0:17:59.\n",
            "  Batch 2,720  of  5,400.    Elapsed: 0:18:15.\n",
            "  Batch 2,760  of  5,400.    Elapsed: 0:18:31.\n",
            "  Batch 2,800  of  5,400.    Elapsed: 0:18:47.\n",
            "  Batch 2,840  of  5,400.    Elapsed: 0:19:03.\n",
            "  Batch 2,880  of  5,400.    Elapsed: 0:19:19.\n",
            "  Batch 2,920  of  5,400.    Elapsed: 0:19:35.\n",
            "  Batch 2,960  of  5,400.    Elapsed: 0:19:51.\n",
            "  Batch 3,000  of  5,400.    Elapsed: 0:20:07.\n",
            "  Batch 3,040  of  5,400.    Elapsed: 0:20:23.\n",
            "  Batch 3,080  of  5,400.    Elapsed: 0:20:40.\n",
            "  Batch 3,120  of  5,400.    Elapsed: 0:20:56.\n",
            "  Batch 3,160  of  5,400.    Elapsed: 0:21:12.\n",
            "  Batch 3,200  of  5,400.    Elapsed: 0:21:28.\n",
            "  Batch 3,240  of  5,400.    Elapsed: 0:21:44.\n",
            "  Batch 3,280  of  5,400.    Elapsed: 0:22:00.\n",
            "  Batch 3,320  of  5,400.    Elapsed: 0:22:16.\n",
            "  Batch 3,360  of  5,400.    Elapsed: 0:22:32.\n",
            "  Batch 3,400  of  5,400.    Elapsed: 0:22:48.\n",
            "  Batch 3,440  of  5,400.    Elapsed: 0:23:04.\n",
            "  Batch 3,480  of  5,400.    Elapsed: 0:23:20.\n",
            "  Batch 3,520  of  5,400.    Elapsed: 0:23:37.\n",
            "  Batch 3,560  of  5,400.    Elapsed: 0:23:53.\n",
            "  Batch 3,600  of  5,400.    Elapsed: 0:24:09.\n",
            "  Batch 3,640  of  5,400.    Elapsed: 0:24:25.\n",
            "  Batch 3,680  of  5,400.    Elapsed: 0:24:41.\n",
            "  Batch 3,720  of  5,400.    Elapsed: 0:24:58.\n",
            "  Batch 3,760  of  5,400.    Elapsed: 0:25:14.\n",
            "  Batch 3,800  of  5,400.    Elapsed: 0:25:30.\n",
            "  Batch 3,840  of  5,400.    Elapsed: 0:25:46.\n",
            "  Batch 3,880  of  5,400.    Elapsed: 0:26:02.\n",
            "  Batch 3,920  of  5,400.    Elapsed: 0:26:18.\n",
            "  Batch 3,960  of  5,400.    Elapsed: 0:26:34.\n",
            "  Batch 4,000  of  5,400.    Elapsed: 0:26:51.\n",
            "  Batch 4,040  of  5,400.    Elapsed: 0:27:07.\n",
            "  Batch 4,080  of  5,400.    Elapsed: 0:27:23.\n",
            "  Batch 4,120  of  5,400.    Elapsed: 0:27:39.\n",
            "  Batch 4,160  of  5,400.    Elapsed: 0:27:55.\n",
            "  Batch 4,200  of  5,400.    Elapsed: 0:28:11.\n",
            "  Batch 4,240  of  5,400.    Elapsed: 0:28:27.\n",
            "  Batch 4,280  of  5,400.    Elapsed: 0:28:43.\n",
            "  Batch 4,320  of  5,400.    Elapsed: 0:29:00.\n",
            "  Batch 4,360  of  5,400.    Elapsed: 0:29:16.\n",
            "  Batch 4,400  of  5,400.    Elapsed: 0:29:32.\n",
            "  Batch 4,440  of  5,400.    Elapsed: 0:29:48.\n",
            "  Batch 4,480  of  5,400.    Elapsed: 0:30:04.\n",
            "  Batch 4,520  of  5,400.    Elapsed: 0:30:20.\n",
            "  Batch 4,560  of  5,400.    Elapsed: 0:30:36.\n",
            "  Batch 4,600  of  5,400.    Elapsed: 0:30:52.\n",
            "  Batch 4,640  of  5,400.    Elapsed: 0:31:09.\n",
            "  Batch 4,680  of  5,400.    Elapsed: 0:31:25.\n",
            "  Batch 4,720  of  5,400.    Elapsed: 0:31:41.\n",
            "  Batch 4,760  of  5,400.    Elapsed: 0:31:57.\n",
            "  Batch 4,800  of  5,400.    Elapsed: 0:32:13.\n",
            "  Batch 4,840  of  5,400.    Elapsed: 0:32:29.\n",
            "  Batch 4,880  of  5,400.    Elapsed: 0:32:46.\n",
            "  Batch 4,920  of  5,400.    Elapsed: 0:33:02.\n",
            "  Batch 4,960  of  5,400.    Elapsed: 0:33:18.\n",
            "  Batch 5,000  of  5,400.    Elapsed: 0:33:34.\n",
            "  Batch 5,040  of  5,400.    Elapsed: 0:33:50.\n",
            "  Batch 5,080  of  5,400.    Elapsed: 0:34:06.\n",
            "  Batch 5,120  of  5,400.    Elapsed: 0:34:22.\n",
            "  Batch 5,160  of  5,400.    Elapsed: 0:34:39.\n",
            "  Batch 5,200  of  5,400.    Elapsed: 0:34:55.\n",
            "  Batch 5,240  of  5,400.    Elapsed: 0:35:11.\n",
            "  Batch 5,280  of  5,400.    Elapsed: 0:35:27.\n",
            "  Batch 5,320  of  5,400.    Elapsed: 0:35:43.\n",
            "  Batch 5,360  of  5,400.    Elapsed: 0:35:59.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:36:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:01:25\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C4xOKg-xoOz",
        "colab_type": "text"
      },
      "source": [
        "## 评估\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g60zz_LfPsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a47e4089-0e88-4ed5-9483-79cfe9a264d5"
      },
      "source": [
        "t0 = time.time()\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "    \n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():        \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "    \n",
        "    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "    # values prior to applying an activation function like the softmax.\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Calculate the accuracy for this batch of test sentences.\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    # Accumulate the total accuracy.\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    # Track the number of batches\n",
        "    nb_eval_steps += 1\n",
        "print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"  Test took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.9787\n",
            "  Test took: 0:03:32\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}